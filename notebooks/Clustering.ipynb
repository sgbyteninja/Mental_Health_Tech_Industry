{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pycountry\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "import gower\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import f_oneway\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the PCA data and Gower Distance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Path settings csv file\n",
    "csv_filepath = '../data/interim/pca.csv'\n",
    "\n",
    "# Loading the csv file\n",
    "data_pca = pd.read_csv(csv_filepath)\n",
    "print(\"Data loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Path settings csv file\n",
    "csv_filepath = '../data/interim/gower.csv'\n",
    "\n",
    "# Loading the csv file\n",
    "data_gower = pd.read_csv(csv_filepath)\n",
    "print(\"Data loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch Optimal Hyperparamters for PCA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0.5\n",
    "end = 30\n",
    "step = 0.5\n",
    "eps_values = np.arange(start, end, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 2\n",
    "end = 50\n",
    "step = 2\n",
    "min_samples = np.arange(start, end, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = pd.DataFrame(columns = [\"eps_value\", \"min_sample\", \"score\", \"n_clusters\", \"cluster_types\"])\n",
    "\n",
    "for eps_value in eps_values:\n",
    "    for min_sample in min_samples:\n",
    "    \n",
    "    # Perform DBSCAN clustering\n",
    "        dbscan_cluster = DBSCAN(eps=eps_value, min_samples=min_sample, metric=\"euclidean\")\n",
    "        labels = dbscan_cluster.fit_predict(data_pca)\n",
    "        n_clusters = len(set(labels))\n",
    "        cluster_types = set(labels)\n",
    "        try:\n",
    "            score = metrics.silhouette_score(data_pca, labels)\n",
    "        except ValueError:\n",
    "            score = 0\n",
    "        new_row = {'eps_value': eps_value, 'min_sample': min_sample, 'score' : score, 'n_clusters' : n_clusters, 'cluster_types': cluster_types}\n",
    "        # Adding the new row without using append\n",
    "        df_analysis.loc[len(df_analysis)] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eps_value</th>\n",
       "      <th>min_sample</th>\n",
       "      <th>score</th>\n",
       "      <th>n_clusters</th>\n",
       "      <th>cluster_types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>3.0</td>\n",
       "      <td>34</td>\n",
       "      <td>0.105063</td>\n",
       "      <td>3</td>\n",
       "      <td>{0, 1, -1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     eps_value  min_sample     score  n_clusters cluster_types\n",
       "136        3.0          34  0.105063           3    {0, 1, -1}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I want at least 3 Clusters so I query the for n > 2\n",
    "df_analysis_3 = df_analysis.query(\"n_clusters > 2\")\n",
    "\n",
    "# Determine the best silhouette scores for clusters that are bigger than 2\n",
    "best_clusters = df_analysis_3[df_analysis_3['score'] == df_analysis_3['score'].max()]\n",
    "best_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch Optimal Hyperparamters for Gower Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0.1\n",
    "end = 1\n",
    "step = 0.02\n",
    "eps_values = np.arange(start, end, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 2\n",
    "end = 50\n",
    "step = 2\n",
    "min_samples = np.arange(start, end, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis_gower = pd.DataFrame(columns = [\"eps_value\", \"min_sample\", \"score\", \"n_clusters\", \"cluster_types\"])\n",
    "\n",
    "for eps_value in eps_values:\n",
    "    for min_sample in min_samples:\n",
    "    \n",
    "    # Perform DBSCAN clustering\n",
    "        dbscan_cluster = DBSCAN(eps=eps_value, min_samples=min_sample, metric=\"precomputed\")\n",
    "        labels = dbscan_cluster.fit_predict(data_gower)\n",
    "        n_clusters = len(set(labels))\n",
    "        cluster_types = set(labels)\n",
    "        try:\n",
    "            score = metrics.silhouette_score(data_gower, labels)\n",
    "        except ValueError:\n",
    "            score = 0\n",
    "        new_row = {'eps_value': eps_value, 'min_sample': min_sample, 'score' : score, 'n_clusters' : n_clusters, 'cluster_types': cluster_types}\n",
    "        \n",
    "        # Adding the new row without using append\n",
    "        df_analysis_gower.loc[len(df_analysis_gower)] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eps_value</th>\n",
       "      <th>min_sample</th>\n",
       "      <th>score</th>\n",
       "      <th>n_clusters</th>\n",
       "      <th>cluster_types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.16</td>\n",
       "      <td>24</td>\n",
       "      <td>0.213477</td>\n",
       "      <td>3</td>\n",
       "      <td>{0, 1, -1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    eps_value  min_sample     score  n_clusters cluster_types\n",
       "83       0.16          24  0.213477           3    {0, 1, -1}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I want at least 3 Clusters so I query n > 2\n",
    "df_analysis_gower_3 = df_analysis_gower.query(\"n_clusters > 2\")\n",
    "\n",
    "# Determine the best silhouette scores for clusters that are bigger than 2\n",
    "best_clusters_gower = df_analysis_gower_3[df_analysis_gower_3['score'] == df_analysis_gower_3['score'].max()]\n",
    "best_clusters_gower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering with the set Hyperparameters PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 3\n",
      "Noise points: 525\n",
      "Cluster 0: 555 data points\n",
      "Cluster 1: 353 data points\n"
     ]
    }
   ],
   "source": [
    "# Best parameters\n",
    "best_eps_value = 3.0\n",
    "best_min_sample = 34\n",
    "\n",
    "# Perform DBSCAN clustering with the best parameters\n",
    "best_dbscan_cluster = DBSCAN(eps=best_eps_value, min_samples=best_min_sample, metric=\"euclidean\")\n",
    "cluster_labels = best_dbscan_cluster.fit_predict(data_pca)\n",
    "\n",
    "# Find number of clusters and noise points\n",
    "num_clusters = len(set(cluster_labels)) \n",
    "num_noise_points = list(cluster_labels).count(-1)\n",
    "\n",
    "print(\"Number of clusters:\", num_clusters)\n",
    "\n",
    "# Calculate the number of data points in each cluster\n",
    "unique_clusters, counts = np.unique(cluster_labels, return_counts=True)\n",
    "\n",
    "# Output the number of data points in each cluster\n",
    "for cluster, count in zip(unique_clusters, counts):\n",
    "    if cluster == -1:\n",
    "        print(f\"Noise points: {count}\")\n",
    "    else:\n",
    "        print(f\"Cluster {cluster}: {count} data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gower Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering with the set Hyperparameters Gower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 3\n",
      "Noise points: 487\n",
      "Cluster 0: 564 data points\n",
      "Cluster 1: 382 data points\n"
     ]
    }
   ],
   "source": [
    "# Best parameters\n",
    "best_eps_value = 0.16\n",
    "best_min_sample = 24\n",
    "\n",
    "# Perform DBSCAN clustering with the best parameters\n",
    "best_dbscan_cluster_gower = DBSCAN(eps=best_eps_value, min_samples=best_min_sample, metric=\"precomputed\")\n",
    "cluster_labels_gower = best_dbscan_cluster_gower.fit_predict(data_gower)\n",
    "\n",
    "# Find number of clusters and noise points\n",
    "num_clusters_gower = len(set(cluster_labels_gower)) \n",
    "num_noise_points_gower = list(cluster_labels_gower).count(-1)\n",
    "\n",
    "print(\"Number of clusters:\", num_clusters_gower)\n",
    "\n",
    "# Calculate the number of data points in each cluster\n",
    "unique_clusters_gower, counts_gower = np.unique(cluster_labels_gower, return_counts=True)\n",
    "\n",
    "# Output the number of data points in each cluster\n",
    "for cluster_gower, count_gower in zip(unique_clusters_gower, counts_gower):\n",
    "    if cluster_gower == -1:\n",
    "        print(f\"Noise points: {count_gower}\")\n",
    "    else:\n",
    "        print(f\"Cluster {cluster_gower}: {count_gower} data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison between Gower Clusters and PCA Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original_pca = pd.read_csv('../data/interim/final_df.csv')\n",
    "df_original_pca['clusters'] = cluster_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv('../data/interim/final_df.csv')\n",
    "df_original['clusters'] = cluster_labels_gower\n",
    "df_original.to_csv('../data/processed/df_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original_0 = df_original.query(\"clusters == 0\")\n",
    "df_original_pca_0 = df_original_pca.query(\"clusters == 0\")\n",
    "df_original_1 = df_original.query(\"clusters == 1\")\n",
    "df_original_pca_1 = df_original_pca.query(\"clusters == 1\")\n",
    "df_original_n1 = df_original.query(\"clusters == -1\")\n",
    "df_original_pca_n1 = df_original_pca.query(\"clusters == -1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_similar_values(list1, list2):\n",
    "    # Convert lists to sets for efficient comparison\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    \n",
    "    # Calculate the intersection of the two sets\n",
    "    intersection = set1.intersection(set2)\n",
    "    \n",
    "    # Return the number of elements in the intersection\n",
    "    return len(intersection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.925531914893617\n",
      "0.9405405405405406\n"
     ]
    }
   ],
   "source": [
    "union_cluster_0 = count_similar_values(df_original_0.index, df_original_pca_0.index)\n",
    "print(union_cluster_0 / len(df_original_0.index))\n",
    "print(union_cluster_0 / len(df_original_pca_0.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7041884816753927\n",
      "0.7620396600566572\n"
     ]
    }
   ],
   "source": [
    "union_cluster_1 = count_similar_values(df_original_1.index, df_original_pca_1.index)\n",
    "print(union_cluster_1 / len(df_original_1.index))\n",
    "print(union_cluster_1 / len(df_original_pca_1.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7700205338809035\n",
      "0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "union_cluster_n1 = count_similar_values(df_original_n1.index, df_original_pca_n1.index)\n",
    "print(union_cluster_n1 / len(df_original_n1.index))\n",
    "print(union_cluster_n1 / len(df_original_pca_n1.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen, we achieve a similar result with a dimension reduction using PCA as well as with the help of Gower Distance and a subsequent clustering with DBSCAN. For the sake of simplicity, however, I will only use the results of the data set processed with Gower Distance for the further analysis of the clustering, as this provides a better silhouette score and therefore a better clustering. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsupervised_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
